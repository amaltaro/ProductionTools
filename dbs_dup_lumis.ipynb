{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0e4658",
   "metadata": {},
   "source": [
    "# Debugging duplicate lumi sections in central production\n",
    "\n",
    "Given a workflow name, retrieve its description from ReqMgr2 and fetch a bunch of data from DBS for both the input and output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f89c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import traceback\n",
    "import urllib.parse\n",
    "from pprint import pprint, pformat\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ba22b",
   "metadata": {},
   "source": [
    "## Setup credentials (and hardcoded workflow name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdce32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: update the workflow name\n",
    "wflowName = \"amaltaro_SC_PU_5Steps_June2024_Val_240627_215847_1521\"\n",
    "\n",
    "myCert = \"/Users/amaltar2/Devel/DockerWMCore/certs/servicecert.pem\"\n",
    "myKey = \"/Users/amaltar2/Devel/DockerWMCore/certs/servicekey.pem\"\n",
    "# setup proxy/credentials\n",
    "if os.getenv(\"X509_USER_PROXY\"):\n",
    "    myCert = os.getenv(\"X509_USER_PROXY\")\n",
    "    myKey = os.getenv(\"X509_USER_PROXY\")\n",
    "elif os.getenv(\"X509_USER_CERT\"):\n",
    "    myCert = os.getenv(\"X509_USER_CERT\")\n",
    "    myKey = os.getenv(\"X509_USER_KEY\")\n",
    "if not myCert or not myKey:\n",
    "    print(\"ERROR: you need to export the X509 environment variables\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678dd59d",
   "metadata": {},
   "source": [
    "## Fetch workflows from ReqMgr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c2530d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {\"Content-type\": \"application/json\", \"Accept\": \"application/json\"}\n",
    "reqmgrUrl = \"https://cmsweb-testbed.cern.ch/reqmgr2/data/request\"\n",
    "params = {\"name\": wflowName}\n",
    "data = requests.get(reqmgrUrl, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "if data.ok is True:\n",
    "    data = data.json()['result']\n",
    "data = data[0][wflowName]\n",
    "if \"Step1\" in data:\n",
    "    inputDset = data['Step1'].get('InputDataset', \"\")\n",
    "elif \"Task1\" in data:\n",
    "    inputDset = data['Task1'].get('InputDataset', \"\")\n",
    "else:\n",
    "    inputDset = data['InputDataset']\n",
    "\n",
    "outputDsets = data['OutputDatasets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9b087",
   "metadata": {},
   "source": [
    "## Define DBS functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80332f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsUrl = \"https://cmsweb-testbed.cern.ch/dbs/int/global/DBSReader\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37a0b3",
   "metadata": {},
   "source": [
    "### filesummaries for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0275064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummaries(dset, verbose=False):\n",
    "    dbsFileSummaries = os.path.join(dbsUrl, \"filesummaries\")\n",
    "    params = {\"dataset\": dset, \"validFileOnly\": 1}\n",
    "    #print(f\"Fetching filesummaries information for dataset: {dset}\")\n",
    "    data = requests.get(dbsFileSummaries, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()[0]\n",
    "    if verbose:\n",
    "        print(f\"filesummaries data for dataset {dset} is: \\n{data}\")\n",
    "    return {\"num_block\": data[\"num_block\"], \"num_event\": data[\"num_event\"], \"num_file\": data[\"num_file\"], \"num_lumi\": data[\"num_lumi\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a376aeb",
   "metadata": {},
   "source": [
    "### filesummaries for a list of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cbe819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummariesBlock(block_list, verbose=False):\n",
    "    thisSummary = {\"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "\n",
    "    dbsFileSummaries = os.path.join(dbsUrl, \"filesummaries\")\n",
    "    for block in block_list:\n",
    "        params = {\"block_name\": block, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileSummaries, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()[0]\n",
    "        if verbose:\n",
    "            print(f\"filesummaries data for block {block} is: \\n{data}\")\n",
    "        try:\n",
    "            thisSummary[\"num_block\"] += 1\n",
    "            thisSummary[\"num_event\"] += data[\"num_event\"]\n",
    "            thisSummary[\"num_file\"] += data[\"num_file\"]\n",
    "            thisSummary[\"num_lumi\"] += data[\"num_lumi\"]\n",
    "        except Exception as exc:\n",
    "            msg = f\"Failed to fetch filesummaries for block: {block}. \"\n",
    "            msg += f\"Data retrieved: {data}. Error: {str(exc)}\"\n",
    "            print(msg)\n",
    "    return thisSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d517f6",
   "metadata": {},
   "source": [
    "### filelumis for a list of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2d925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummariesFile(block_list, verbose=False):\n",
    "    thisSummary = {\"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    dbsFileLumis = os.path.join(dbsUrl, \"filelumis\")\n",
    "    for block in block_list:\n",
    "        file_list = []\n",
    "        params = {\"block_name\": block, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileLumis, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()\n",
    "        if verbose:\n",
    "            print(f\"filelumis data for block {block} is: \\n{pformat(data)}\")\n",
    "        for item in data:\n",
    "            try:\n",
    "                if item[\"logical_file_name\"] not in file_list:\n",
    "                    file_list.append(item[\"logical_file_name\"])\n",
    "                    thisSummary[\"num_file\"] += 1\n",
    "                thisSummary[\"num_lumi\"] += 1\n",
    "                thisSummary[\"num_event\"] += 0 if item[\"event_count\"] is None else item[\"event_count\"]\n",
    "            except Exception as exc:\n",
    "                msg = f\"Failed to fetch filelumis for block: {block}. \"\n",
    "                msg += f\"Data retrieved: {item}. Error: {str(exc)}\"\n",
    "                print(msg)\n",
    "        thisSummary[\"num_block\"] += 1\n",
    "    return thisSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f80b3",
   "metadata": {},
   "source": [
    "### blocks for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed4dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlocks(dset, verbose=False):\n",
    "    dbsBlocks = os.path.join(dbsUrl, \"blocks\")\n",
    "    params = {\"dataset\": dset}\n",
    "    #print(f\"Fetching blocks information for dataset: {dset}\")\n",
    "    data = requests.get(dbsBlocks, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()\n",
    "    if verbose:\n",
    "        print(f\"blocks data for dataset {dset} is: \\n{pformat(data)}\")\n",
    "    return [item['block_name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d7b0",
   "metadata": {},
   "source": [
    "### files for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5074ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(dset, verbose=False):\n",
    "    dbsFiles = os.path.join(dbsUrl, \"files\")\n",
    "    params = {\"dataset\": dset, \"validFileOnly\": 1}\n",
    "    #print(f\"Fetching blocks information for dataset: {dset}\")\n",
    "    data = requests.get(dbsFiles, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()\n",
    "    if verbose:\n",
    "        print(f\"files data for dataset {dset} is: \\n{pformat(data)}\")\n",
    "    return [item['logical_file_name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e8111",
   "metadata": {},
   "source": [
    "### list of lumis per file for a list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14fae35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileLumis(files_list, verbose=False, add_dups = False):\n",
    "    resp = {}\n",
    "    dbsFileLumis = os.path.join(dbsUrl, \"filelumis\")\n",
    "    for fname in files_list:\n",
    "        file_list = []\n",
    "        params = {\"logical_file_name\": fname, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileLumis, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()\n",
    "        if verbose:\n",
    "            print(f\"filelumis data for file {fname} is: \\n{pformat(data)}\")\n",
    "        for item in data:\n",
    "            lfn = item['logical_file_name']\n",
    "            lumi = item['lumi_section_num']\n",
    "            run_number = item['run_num']\n",
    "            resp.setdefault(lfn, [])\n",
    "            if (lumi,run_number) in resp[lfn]:\n",
    "                print(f\"ERROR: Run/Lumi {(run_number, lumi)} in LFN {lfn} is duplicate\")\n",
    "                if add_dups:\n",
    "                    resp[lfn].append((lumi,run_number))\n",
    "            else:\n",
    "                resp[lfn].append((lumi,run_number))\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca625f9f",
   "metadata": {},
   "source": [
    "## Fetch summary for each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "775ead16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.2 ms, sys: 11.2 ms, total: 91.4 ms\n",
      "Wall time: 1.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# retrieve information from DBS at 3 levels: dataset, block and files\n",
    "summary = {\"dataset\": [], \"block\": [], \"file\": []}\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"dataset\"].append(thisDict)\n",
    "else:\n",
    "    data = getFileSummaries(inputDset)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"dataset\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    data = getFileSummaries(dset)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"dataset\"].append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b517ef",
   "metadata": {},
   "source": [
    "## Fetch summary for each block in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ea52f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 11.5 ms, total: 144 ms\n",
      "Wall time: 3.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"block\"].append(thisDict)\n",
    "else:\n",
    "    blocks = getBlocks(inputDset)\n",
    "    data = getFileSummariesBlock(blocks)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"block\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesBlock(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"block\"].append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d21f0c",
   "metadata": {},
   "source": [
    "## Fetch summary for each file in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d79d0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"file\"].append(thisDict)\n",
    "else:\n",
    "    blocks = getBlocks(inputDset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"file\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"file\"].append(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08e38b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for: None\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |               0 |             0 |            0 |\n",
      "| num_file  |               0 |             0 |            0 |\n",
      "| num_event |               0 |             0 |            0 |\n",
      "| num_lumi  |               0 |             0 |            0 |\n",
      "\n",
      "Summary for: /RPVStopStopToJets_UDD323_M-2000_TuneCP5_13p6TeV-madgraphMLM-pythia8/Run3Summer22EEDRPremix-EXO_Run3Summer22EEDRPremix_01310_1_SC_PU_5Steps_June2024_Val_Alanv3-v11/AODSIM\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |               1 |             1 |            1 |\n",
      "| num_file  |             240 |           240 |          240 |\n",
      "| num_event |         1514571 |       1514571 |      1514571 |\n",
      "| num_lumi  |            2405 |          2405 |         2405 |\n",
      "\n",
      "Summary for: /RPVStopStopToJets_UDD323_M-2000_TuneCP5_13p6TeV-madgraphMLM-pythia8/Run3Summer22EEMiniAODv4-EXO_Run3Summer22EEMiniAODv4_01191_0_SC_PU_5Steps_June2024_Val_Alanv3-v11/MINIAODSIM\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |               1 |             1 |            1 |\n",
      "| num_file  |              72 |            72 |           72 |\n",
      "| num_event |         1514571 |       1514571 |      1514571 |\n",
      "| num_lumi  |            2405 |          2405 |         2405 |\n",
      "\n",
      "Summary for: /RPVStopStopToJets_UDD323_M-2000_TuneCP5_13p6TeV-madgraphMLM-pythia8/Run3Summer22EENanoAODv12-EXO_Run3Summer22EENanoAODv12_01191_0_SC_PU_5Steps_June2024_Val_Alanv3-v11/NANOAODSIM\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |               1 |             1 |            1 |\n",
      "| num_file  |               4 |             4 |            4 |\n",
      "| num_event |          537513 |        537513 |            0 |\n",
      "| num_lumi  |             854 |           854 |          854 |\n"
     ]
    }
   ],
   "source": [
    "num_datasets = len(summary['dataset'])\n",
    "for i in range(num_datasets):\n",
    "    print(f\"\\nSummary for: {summary['dataset'][i]['dataset_name']}\")\n",
    "    header_table = [\"metric\", \"dataset level\", \"block level\", \"file level\"]\n",
    "    data_table = [[\"num_block\", summary['dataset'][i]['num_block'], summary['block'][i]['num_block'], summary['file'][i]['num_block']],\n",
    "                  [\"num_file\", summary['dataset'][i]['num_file'], summary['block'][i]['num_file'], summary['file'][i]['num_file']],\n",
    "                  [\"num_event\", summary['dataset'][i]['num_event'], summary['block'][i]['num_event'], summary['file'][i]['num_event']],\n",
    "                  [\"num_lumi\", summary['dataset'][i]['num_lumi'], summary['block'][i]['num_lumi'], summary['file'][i]['num_lumi']]]\n",
    "    print(tabulate(data_table, headers=header_table, tablefmt=\"github\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a285eb",
   "metadata": {},
   "source": [
    "## Save this summary in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff1b7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_dbs.json\", \"w\") as fo:\n",
    "    json.dump(summary, fo, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055572",
   "metadata": {},
   "source": [
    "## Fetching list of lumis per file from input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2f2911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of duplicate files and lumis is:\n",
      "Lumi {7} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/f956c9e4-c5ae-4ec6-8c77-71bfc3002cf9.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/5d37dc29-67f7-41fb-8a02-4f4645464938.root\n",
      "Lumi {7} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/5d37dc29-67f7-41fb-8a02-4f4645464938.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/f956c9e4-c5ae-4ec6-8c77-71bfc3002cf9.root\n",
      "Lumi {18} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/8acd7ad5-d887-4a0d-9a94-5b435149a2d6.root\n",
      "Lumi {15} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/70ba64c0-3cb9-4469-9ba6-5e3c0b28612e.root\n",
      "Lumi {4} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/1c9e3f46-8505-45ed-bea2-4e9e84b1a827.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/475ae403-5f5f-4d51-aa6d-f4f3f308d91b.root\n",
      "Lumi {18} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/8acd7ad5-d887-4a0d-9a94-5b435149a2d6.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root\n",
      "Lumi {4} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/475ae403-5f5f-4d51-aa6d-f4f3f308d91b.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/1c9e3f46-8505-45ed-bea2-4e9e84b1a827.root\n",
      "Lumi {15} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/70ba64c0-3cb9-4469-9ba6-5e3c0b28612e.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root\n"
     ]
    }
   ],
   "source": [
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "else:\n",
    "    files = getFiles(inputDset)\n",
    "    #print(f\"List of files: {pformat(files)}\")\n",
    "    data = getFileLumis(files)\n",
    "    #print(\"Map of lumis per file is:\")\n",
    "    #for lfn, lumis in data.items():\n",
    "    #    print(f\"{lfn} contains {sorted(lumis)}\")\n",
    "    print(\"Map of duplicate files and lumis is:\")\n",
    "    for lfn, lumis in data.items():\n",
    "        for lfn2, lumis2 in data.items():\n",
    "            if lfn == lfn2:\n",
    "                continue\n",
    "            dupLumis = set(lumis) & set(lumis2)\n",
    "            if dupLumis:\n",
    "                print(f\"Lumi {dupLumis} is common between {lfn} and {lfn2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b7ff1",
   "metadata": {},
   "source": [
    "## Fetching list of lumis for one of the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"file\"].append(data)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
