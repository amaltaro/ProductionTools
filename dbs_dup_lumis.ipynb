{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0e4658",
   "metadata": {},
   "source": [
    "# Debugging duplicate lumi sections in central production\n",
    "\n",
    "Given a workflow name, retrieve its description from ReqMgr2 and fetch a bunch of data from DBS for both the input and output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56f89c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import traceback\n",
    "import urllib.parse\n",
    "from pprint import pprint, pformat\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "myCert = \"/Users/amaltar2/Devel/DockerWMCore/certs/servicecert.pem\"\n",
    "myKey = \"/Users/amaltar2/Devel/DockerWMCore/certs/servicekey.pem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678dd59d",
   "metadata": {},
   "source": [
    "## Fetch workflows from ReqMgr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85c2530d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: update the workflow name\n",
    "wflowName = \"cmsunified_task_EXO-Run3Summer22GS-00318__v1_T_230302_175516_6976\"\n",
    "\n",
    "headers = {\"Content-type\": \"application/json\", \"Accept\": \"application/json\"}\n",
    "reqmgrUrl = \"https://cmsweb.cern.ch/reqmgr2/data/request\"\n",
    "params = {\"name\": wflowName}\n",
    "data = requests.get(reqmgrUrl, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "if data.ok is True:\n",
    "    data = data.json()['result']\n",
    "data = data[0][wflowName]\n",
    "if \"Step1\" in data:\n",
    "    inputDset = data['Step1'].get('InputDataset', \"\")\n",
    "elif \"Task1\" in data:\n",
    "    inputDset = data['Task1'].get('InputDataset', \"\")\n",
    "else:\n",
    "    inputDset = data['InputDataset']\n",
    "\n",
    "outputDsets = data['OutputDatasets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9b087",
   "metadata": {},
   "source": [
    "## Define DBS functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80332f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsUrl = \"https://cmsweb.cern.ch/dbs/prod/global/DBSReader\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37a0b3",
   "metadata": {},
   "source": [
    "### filesummaries for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0275064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummaries(dset, verbose=False):\n",
    "    dbsFileSummaries = os.path.join(dbsUrl, \"filesummaries\")\n",
    "    params = {\"dataset\": dset, \"validFileOnly\": 1}\n",
    "    #print(f\"Fetching filesummaries information for dataset: {dset}\")\n",
    "    data = requests.get(dbsFileSummaries, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()[0]\n",
    "    if verbose:\n",
    "        print(f\"filesummaries data for dataset {dset} is: \\n{data}\")\n",
    "    return {\"num_block\": data[\"num_block\"], \"num_event\": data[\"num_event\"], \"num_file\": data[\"num_file\"], \"num_lumi\": data[\"num_lumi\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a376aeb",
   "metadata": {},
   "source": [
    "### filesummaries for a list of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6cbe819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummariesBlock(block_list, verbose=False):\n",
    "    thisSummary = {\"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "\n",
    "    dbsFileSummaries = os.path.join(dbsUrl, \"filesummaries\")\n",
    "    for block in block_list:\n",
    "        params = {\"block_name\": block, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileSummaries, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()[0]\n",
    "        if verbose:\n",
    "            print(f\"filesummaries data for block {block} is: \\n{data}\")\n",
    "        try:\n",
    "            thisSummary[\"num_block\"] += 1\n",
    "            thisSummary[\"num_event\"] += data[\"num_event\"]\n",
    "            thisSummary[\"num_file\"] += data[\"num_file\"]\n",
    "            thisSummary[\"num_lumi\"] += data[\"num_lumi\"]\n",
    "        except Exception as exc:\n",
    "            msg = f\"Failed to fetch filesummaries for block: {block}. \"\n",
    "            msg += f\"Data retrieved: {data}. Error: {str(exc)}\"\n",
    "            print(msg)\n",
    "    return thisSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d517f6",
   "metadata": {},
   "source": [
    "### filelumis for a list of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b2d925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileSummariesFile(block_list, verbose=False):\n",
    "    thisSummary = {\"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    dbsFileLumis = os.path.join(dbsUrl, \"filelumis\")\n",
    "    for block in block_list:\n",
    "        file_list = []\n",
    "        params = {\"block_name\": block, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileLumis, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()\n",
    "        if verbose:\n",
    "            print(f\"filelumis data for block {block} is: \\n{pformat(data)}\")\n",
    "        for item in data:\n",
    "            try:\n",
    "                if item[\"logical_file_name\"] not in file_list:\n",
    "                    file_list.append(item[\"logical_file_name\"])\n",
    "                    thisSummary[\"num_file\"] += 1\n",
    "                thisSummary[\"num_lumi\"] += 1\n",
    "                thisSummary[\"num_event\"] += item[\"event_count\"]\n",
    "            except Exception as exc:\n",
    "                msg = f\"Failed to fetch filelumis for block: {block}. \"\n",
    "                msg += f\"Data retrieved: {item}. Error: {str(exc)}\"\n",
    "                print(msg)\n",
    "        thisSummary[\"num_block\"] += 1\n",
    "    return thisSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f80b3",
   "metadata": {},
   "source": [
    "### blocks for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ed4dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlocks(dset, verbose=False):\n",
    "    dbsBlocks = os.path.join(dbsUrl, \"blocks\")\n",
    "    params = {\"dataset\": dset}\n",
    "    #print(f\"Fetching blocks information for dataset: {dset}\")\n",
    "    data = requests.get(dbsBlocks, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()\n",
    "    if verbose:\n",
    "        print(f\"blocks data for dataset {dset} is: \\n{pformat(data)}\")\n",
    "    return [item['block_name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d7b0",
   "metadata": {},
   "source": [
    "### files for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5074ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(dset, verbose=False):\n",
    "    dbsFiles = os.path.join(dbsUrl, \"files\")\n",
    "    params = {\"dataset\": dset, \"validFileOnly\": 1}\n",
    "    #print(f\"Fetching blocks information for dataset: {dset}\")\n",
    "    data = requests.get(dbsFiles, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "    if data.ok is True:\n",
    "        data = data.json()\n",
    "    if verbose:\n",
    "        print(f\"files data for dataset {dset} is: \\n{pformat(data)}\")\n",
    "    return [item['logical_file_name'] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e8111",
   "metadata": {},
   "source": [
    "### list of lumis per file for a list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14fae35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileLumis(files_list, verbose=False):\n",
    "    resp = {}\n",
    "    dbsFileLumis = os.path.join(dbsUrl, \"filelumis\")\n",
    "    for fname in files_list:\n",
    "        file_list = []\n",
    "        params = {\"logical_file_name\": fname, \"validFileOnly\": 1}\n",
    "        #print(f\"Fetching filesummaries information for block: {block}\")\n",
    "        data = requests.get(dbsFileLumis, params=params, headers=headers, cert=(myCert, myKey), verify=False)\n",
    "        if data.ok is True:\n",
    "            data = data.json()\n",
    "        if verbose:\n",
    "            print(f\"filelumis data for file {fname} is: \\n{pformat(data)}\")\n",
    "        for item in data:\n",
    "            lfn = item['logical_file_name']\n",
    "            lumi = item['lumi_section_num']\n",
    "            resp.setdefault(lfn, [])\n",
    "            if lumi in resp[lfn]:\n",
    "                print(f\"ERROR: Lumi {lumi} in LFN {lfn} is duplicate\")\n",
    "            else:\n",
    "                resp[lfn].append(lumi)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca625f9f",
   "metadata": {},
   "source": [
    "## Fetch summary for each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "775ead16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.2 ms, sys: 11.2 ms, total: 81.5 ms\n",
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# retrieve information from DBS at 3 levels: dataset, block and files\n",
    "summary = {\"dataset\": [], \"block\": [], \"file\": []}\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"dataset\"].append(thisDict)\n",
    "else:\n",
    "    data = getFileSummaries(inputDset)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"dataset\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    data = getFileSummaries(dset)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"dataset\"].append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b517ef",
   "metadata": {},
   "source": [
    "## Fetch summary for each block in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ea52f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 82.6 ms, total: 1.37 s\n",
      "Wall time: 22.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"block\"].append(thisDict)\n",
    "else:\n",
    "    blocks = getBlocks(inputDset)\n",
    "    data = getFileSummariesBlock(blocks)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"block\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesBlock(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"block\"].append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d21f0c",
   "metadata": {},
   "source": [
    "## Fetch summary for each file in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d79d0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:33<00:00, 16.73s/it]\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "    summary[\"file\"].append(thisDict)\n",
    "else:\n",
    "    blocks = getBlocks(inputDset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = inputDset\n",
    "    summary[\"file\"].append(data)\n",
    "\n",
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"file\"].append(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08e38b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for: None\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |               0 |             0 |            0 |\n",
      "| num_file  |               0 |             0 |            0 |\n",
      "| num_event |               0 |             0 |            0 |\n",
      "| num_lumi  |               0 |             0 |            0 |\n",
      "\n",
      "Summary for: /ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/Run3Summer22DRPremix-124X_mcRun3_2022_realistic_v12-v2/AODSIM\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |              16 |            16 |           16 |\n",
      "| num_file  |              14 |            14 |           14 |\n",
      "| num_event |           12896 |         12896 |        12896 |\n",
      "| num_lumi  |              15 |            19 |           19 |\n",
      "\n",
      "Summary for: /ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/Run3Summer22MiniAODv3-124X_mcRun3_2022_realistic_v12-v2/MINIAODSIM\n",
      "| metric    |   dataset level |   block level |   file level |\n",
      "|-----------|-----------------|---------------|--------------|\n",
      "| num_block |              16 |            16 |           16 |\n",
      "| num_file  |              17 |            17 |           17 |\n",
      "| num_event |           16456 |         16456 |        16456 |\n",
      "| num_lumi  |              18 |            24 |           24 |\n"
     ]
    }
   ],
   "source": [
    "num_datasets = len(summary['dataset'])\n",
    "for i in range(num_datasets):\n",
    "    print(f\"\\nSummary for: {summary['dataset'][i]['dataset_name']}\")\n",
    "    header_table = [\"metric\", \"dataset level\", \"block level\", \"file level\"]\n",
    "    data_table = [[\"num_block\", summary['dataset'][i]['num_block'], summary['block'][i]['num_block'], summary['file'][i]['num_block']],\n",
    "                  [\"num_file\", summary['dataset'][i]['num_file'], summary['block'][i]['num_file'], summary['file'][i]['num_file']],\n",
    "                  [\"num_event\", summary['dataset'][i]['num_event'], summary['block'][i]['num_event'], summary['file'][i]['num_event']],\n",
    "                  [\"num_lumi\", summary['dataset'][i]['num_lumi'], summary['block'][i]['num_lumi'], summary['file'][i]['num_lumi']]]\n",
    "    print(tabulate(data_table, headers=header_table, tablefmt=\"github\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a285eb",
   "metadata": {},
   "source": [
    "## Save this summary in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff1b7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_dbs.json\", \"w\") as fo:\n",
    "    json.dump(summary, fo, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055572",
   "metadata": {},
   "source": [
    "## Fetching list of lumis per file from input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2f2911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of duplicate files and lumis is:\n",
      "Lumi {7} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/f956c9e4-c5ae-4ec6-8c77-71bfc3002cf9.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/5d37dc29-67f7-41fb-8a02-4f4645464938.root\n",
      "Lumi {7} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/5d37dc29-67f7-41fb-8a02-4f4645464938.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/f956c9e4-c5ae-4ec6-8c77-71bfc3002cf9.root\n",
      "Lumi {18} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/8acd7ad5-d887-4a0d-9a94-5b435149a2d6.root\n",
      "Lumi {15} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/70ba64c0-3cb9-4469-9ba6-5e3c0b28612e.root\n",
      "Lumi {4} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/1c9e3f46-8505-45ed-bea2-4e9e84b1a827.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/475ae403-5f5f-4d51-aa6d-f4f3f308d91b.root\n",
      "Lumi {18} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/8acd7ad5-d887-4a0d-9a94-5b435149a2d6.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root\n",
      "Lumi {4} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/475ae403-5f5f-4d51-aa6d-f4f3f308d91b.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/1c9e3f46-8505-45ed-bea2-4e9e84b1a827.root\n",
      "Lumi {15} is common between /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/70ba64c0-3cb9-4469-9ba6-5e3c0b28612e.root and /store/mc/Run3Summer22DRPremix/ZPrime2DarkPhoton_HMass-1000_DPMass-0p3_TuneCP5_13p6TeV-pythia8/AODSIM/124X_mcRun3_2022_realistic_v12-v2/2810000/9cf8c597-1746-4709-8d90-e0286755ec42.root\n"
     ]
    }
   ],
   "source": [
    "if not inputDset:\n",
    "    thisDict = {\"dataset_name\": None, \"num_block\": 0, \"num_event\": 0, \"num_file\": 0, \"num_lumi\": 0}\n",
    "else:\n",
    "    files = getFiles(inputDset)\n",
    "    #print(f\"List of files: {pformat(files)}\")\n",
    "    data = getFileLumis(files)\n",
    "    #print(\"Map of lumis per file is:\")\n",
    "    #for lfn, lumis in data.items():\n",
    "    #    print(f\"{lfn} contains {sorted(lumis)}\")\n",
    "    print(\"Map of duplicate files and lumis is:\")\n",
    "    for lfn, lumis in data.items():\n",
    "        for lfn2, lumis2 in data.items():\n",
    "            if lfn == lfn2:\n",
    "                continue\n",
    "            dupLumis = set(lumis) & set(lumis2)\n",
    "            if dupLumis:\n",
    "                print(f\"Lumi {dupLumis} is common between {lfn} and {lfn2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b7ff1",
   "metadata": {},
   "source": [
    "## Fetching list of lumis for one of the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in tqdm(outputDsets, total=len(outputDsets)):\n",
    "    blocks = getBlocks(dset)\n",
    "    data = getFileSummariesFile(blocks)\n",
    "    data[\"dataset_name\"] = dset\n",
    "    summary[\"file\"].append(data)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
